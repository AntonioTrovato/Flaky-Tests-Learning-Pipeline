{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "416cee15-b5e3-4f58-9aa7-adce40461a4b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# FlakyPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Business Understanding\n",
    "Ogni volta che viene scritto un nuovo codice per sviluppare o aggiornare software, una pagina Web o un'app, deve essere testato durante tutto il processo di sviluppo per assicurarsi che l'applicazione faccia ciò che dovrebbe fare quando viene rilasciata per l'uso. Logicamente, quando viene sottoposto allo stesso test più e più volte, il codice produrrà lo stesso risultato: l'applicazione funzionerà correttamente ogni volta, superando così il test, o non funzionerà correttamente ogni volta, fallendo così il test.\n",
    "\n",
    "Tuttavia, apparentemente a caso, occasionalmente lo stesso test produrrà risultati diversi. A volte mostrerà che il codice ha superato il test e l'applicazione ha funzionato come pianificato, e talvolta mostrerà che il codice non ha superato il test e non ha funzionato come previsto. Quando succede questo, il test è considerato flaky.\n",
    "\n",
    "I flaky possono essere causati da vari fattori:\n",
    "1. un problema con il codice appena scritto\n",
    "2. un problema con il test stesso\n",
    "3. alcuni fattori esterni che compromettono i risultati del test\n",
    "\n",
    "Non è sempre facile individuare tali testi, può capitare che eseguiamo un test 10000 volte ed avremo sempre lo stesso risultato, ma se eseguissimo il test un ulteriore volta avremo un risultato diverso. Lo scopo di \"FlakyPipeline\" e di utilizzare il machine learnig per determinare se un test può essere considerato flaky oppure no.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Understanding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Utente\\\\Documents\\\\GitHub\\\\Flaky Tests Learning Pipeline\\\\flakeFlagger.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 12>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      8\u001B[0m     csv_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(current_directory, datasetname)\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pandas\u001B[38;5;241m.\u001B[39mread_csv(csv_path)\n\u001B[1;32m---> 12\u001B[0m dataset\u001B[38;5;241m=\u001B[39m\u001B[43mloadingDataSet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATASET_NAME\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m dataset\u001B[38;5;241m.\u001B[39mhead()\n",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36mloadingDataSet\u001B[1;34m(datasetname)\u001B[0m\n\u001B[0;32m      7\u001B[0m current_directory\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mgetcwd()\n\u001B[0;32m      8\u001B[0m csv_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(current_directory, datasetname)\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpandas\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcsv_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Flaky Tests Learning Pipeline\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[0;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[0;32m    310\u001B[0m     )\n\u001B[1;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Flaky Tests Learning Pipeline\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    665\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    666\u001B[0m     dialect,\n\u001B[0;32m    667\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    676\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    677\u001B[0m )\n\u001B[0;32m    678\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 680\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Flaky Tests Learning Pipeline\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    572\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    574\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 575\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    578\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Flaky Tests Learning Pipeline\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    930\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    932\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 933\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Flaky Tests Learning Pipeline\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1213\u001B[0m     mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1214\u001B[0m \u001B[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001B[39;00m\n\u001B[0;32m   1215\u001B[0m \u001B[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001B[39;00m\n\u001B[0;32m   1216\u001B[0m \u001B[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001B[39;00m\n\u001B[1;32m-> 1217\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[call-overload]\u001B[39;49;00m\n\u001B[0;32m   1218\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1219\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1220\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1221\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1222\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1224\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1225\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1226\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1227\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1228\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Flaky Tests Learning Pipeline\\venv\\lib\\site-packages\\pandas\\io\\common.py:789\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    784\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    785\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    786\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    787\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    788\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 789\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    790\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    791\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    792\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    793\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    794\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    795\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    797\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    798\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Utente\\\\Documents\\\\GitHub\\\\Flaky Tests Learning Pipeline\\\\flakeFlagger.csv'"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import os\n",
    "\n",
    "DATASET_NAME='flakeFlagger.csv'\n",
    "\n",
    "def loadingDataSet(datasetname):\n",
    "    current_directory=os.getcwd()\n",
    "    csv_path = os.path.join(current_directory, datasetname)\n",
    "    return pandas.read_csv(csv_path)\n",
    "\n",
    "\n",
    "dataset=loadingDataSet(DATASET_NAME)\n",
    "dataset.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "| Features | Descrizione |\n",
    "| --- | --- | \n",
    "| Id ||\n",
    "| NameProject | |\n",
    "| TestCase | |\n",
    "| tloc | Numero di righe di codice della test suit |\n",
    "| tmcCabe | Somma delle complessità ciclomatiche di tutti i metodi di una classe |\n",
    "| assertionDensity | Percentuali di asserzioni presenti nella test suit |\n",
    "| assertionRoulette | Metrica che indica se il test ha più di una asserzione non documentata |\n",
    "| mysteryGuest | Metrica che indica se il test utilizza una risorsa esterna (es: database,file ...) |\n",
    "| eagerTest | Metricha che indica se un test invoca diversi metodi dell'oggetto di produzione.|\n",
    "| sensitiveEquality | Indica se il metodo toString e utilizzado nel test|\n",
    "| resourceOptimism | Metodo che fa assunzioni ottimistiche sull'esistenza di una risorsa (es file) utilizzata all'interno di esso |\n",
    "| conditionalTestLogic ||\n",
    "| fireAndForget | Test che termina prematuramente in quanto non aspetta le risposte delle chiamate esterne |\n",
    "| loc | line di codice comprendendo anche i commenti | \n",
    "| locm2 |  | \n",
    "| locm5 |  |\n",
    "| cbo | Numero di dipendenze di una classe con altre classi |\n",
    "| wmc | Somma delle complessità ciclomatiche di tutti i metodi di una classe | \n",
    "| rfc | Numero di metodi (compresi quelli ereditari) che possono essere chiamati da altre classi |\n",
    "| mpc |  |\n",
    "| halsteadVocabulary | Gaussian |\n",
    "| halsteadLength | Numero totale di operatori e operandi distinti un una funzione |\n",
    "| halsteadVolume | Memoria (in bit) necessaria per memorizzare il programma |\n",
    "| classDataShouldBePrivate | Classe che espone i suoi attributi, violando il principio dell'information hiding. |\n",
    "| complexClass | Complessita ciclomatica di una classe, ovvero il numero di cammini linearmente indipendeti all'interno della classe|\n",
    "| functionalDecomposition | Metrica che indica se in una classe ereditarietà e polimorfismo sono utilizzate in modo sbagliato. |\n",
    "| godClass | Classe di grandi dimensioni che implementa diverse responsabilità |\n",
    "| spaghettiCode | Classe non possiedie una struttura coerente ad esempio un metodo eccessivamente lungo che non possiede parametri |\n",
    "| isFlaky | Booleano che indica se il test è flaky oppure no |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.describe(include='all')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Il dataset da utlizzare non possiede valori nulli, pertando non sara necessario eseguire operazioni di \"Data Cleaning\" tuttavia, risulta essere sbilanciato in quanto sono presenti 9115/9785 campioni di test la cui label isFlaky e False.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Partizionamento DataSet\n",
    "Prima di manipolare il dataset, esso verra diviso in train-set(80%) e test-set(20%).\n",
    "Il tain-set sarà utlizzato per l'identificazione di un modello predittivo, mentre il test-set verrà utilizzato per testare l'algoritmo di machine learnin.\n",
    "Per il partizionamento del dataset viene adottato un campionamento statificato in modo tale da avere le stesse proporzioni di test flaky (False, True) tra il dataset di test e quello di training\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "for train_index_stratified,test_index_stratified in split.split(dataset,dataset['isFlaky']):\n",
    "    train_set=dataset.loc[train_index_stratified]\n",
    "    test_set=dataset.loc[test_index_stratified]\n",
    "\n",
    "print(\"Dimensione Train-set \",len(train_set))\n",
    "print(\"Dimensione Test-set \",len(test_set))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparazione dei dati per l'estrazione di un modello\n",
    "In tale fase andre ad effettuare delle trasformazioni sul dataset andando ad aumentare la qualità dei dati in modo da ottimizzare l'estrazione di un modello predittivo.\n",
    "La prima operazione che si dovrebbe effettuare e quella del \"Data Cleaning\", tuttavia tale passaggio può essere saltato in quanto il nel dataset non sono presenti valori nulli. \n",
    "Il passo successivo e quello della \"Feature Construction\". \n",
    "In tale fase si andremo a manipolare le caratteristiche rendendole piu efficienti per un compito di data maining. L'operazione che verra eseguira sara la riduzione della dimensionalità, in cui si cerchera di ridurre le caratteristiche mantenendo quante più informazioni possibili. Esistono varie tecniche per ridurre la dimensionalità tuttavia si e preferito utilizzare la PCA (Analisi delle componenti principali)\n",
    "\n",
    "\n",
    "Tuttavia la PCA risulta esser più efficiente su dati standardizzati. Per tale motivo verra eseguita la fase di Feature scaling.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature Scaling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "train_dataset=train_set.drop(['id','nameProject','testCase','isFlaky'],axis=1)\n",
    "train_dataset_lable=train_set['isFlaky']\n",
    "\n",
    "#Standardizzo il dataset\n",
    "sc=StandardScaler()\n",
    "X_train_dataset=sc.fit_transform(train_dataset)\n",
    "print('Train-Set Non Standardizzato')\n",
    "train_dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Train-Set Standardizzato')\n",
    "sc_train_dataset=pandas.DataFrame(X_train_dataset,columns=train_dataset.columns)\n",
    "sc_train_dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature Construction PCA (Principal Component Analysis)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "L’analisi PCA ci aiuta a identificare gli schemi presenti nei dati, sulla base della correlazione fra le caratteristiche. In\n",
    "estrema sintesi, l’analisi PCA mira a trovare le direzioni di massima varianza all’interno di dati a elevata dimensionalità, per poi proiettarli in\n",
    "un nuovo sottospazio avente dimensioni uguali (nel caso in cui le feature sono indipendenti tra loro) o inferiori rispetto all’originale.\n",
    "Per tale problema si ritiene che conservare almeno l'80% di varianza non comportera nessun problema per l'estrazione di un modello con buone prestazioni.\n",
    "Pertano il grafico seguente mostrera la varianza cumulativa in rapporto al numero di componenti."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#Costruisco la matrice di covarianza\n",
    "cov_mat=np.cov(X_train_dataset.T) \n",
    "#Decompongo la matrice di covarianza in un vettore composto dagli autovalori e i corrispondenti autovalori conservati come colonne in una matrice 25x25\n",
    "\n",
    "eigen_vals, eigen_vecs=np.linalg.eig(cov_mat)\n",
    "tot=sum(eigen_vals)\n",
    "var_exp=[(i/tot) for i in sorted(eigen_vals,reverse=True)]\n",
    "cum_var_exp=np.cumsum(var_exp)\n",
    "plt.bar(range(1,26),var_exp,alpha=0.5,align='center',label='Varianza Individuale')\n",
    "plt.step(range(1,26),cum_var_exp,where='mid',label='Varianza Comulativa')\n",
    "plt.ylabel('Variance Ratio')\n",
    "plt.xlabel('Numero Componenti')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=10)\n",
    "principalCompontent=pca.fit_transform(X_train_dataset)\n",
    "pca_train_dataset=pandas.DataFrame(principalCompontent,columns=['Principal Component 1','Principal Component 2','Principal Component  3',\n",
    "                                                                'Principal Component 4','Principal Component 5','Principal Component 6',\n",
    "                                                                'Principal Component 7','Principal Component 8','Principal Component 9',\n",
    "                                                                'Principal Component 10'])\n",
    "\n",
    "\n",
    "sum=0\n",
    "for variance in pca.explained_variance_ratio_:\n",
    "    sum=variance+sum\n",
    "\n",
    "print(\"Varianza totale con 10 componenti: \",sum)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Balancing\n",
    "Come riportato nella fase di data understanding il dataset utilizzato, risulta essere fortemente bilanciato e i test flaky scarseggiano. Proprio quest ultima caratteristica puo risultare un problema nella fase di estrazione di un modello capace di predirre correttamente se un test e flaky oppure no. Pertanto il dataset verra bilanciato utilizzando SMOTE."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "plt.title('Dataset non bilanciato')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.scatter(pca_train_dataset.iloc[:, 0], pca_train_dataset.iloc[:, 1], marker='o', c=train_dataset_lable,\n",
    "           s=25, edgecolor='k', cmap=plt.cm.coolwarm)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sm=SMOTE(sampling_strategy='auto', k_neighbors=1,random_state=42)\n",
    "X_train_dataset,Y_train_dataset=sm.fit_resample(pca_train_dataset,train_dataset_lable)\n",
    "\n",
    "plt.title('Dataset bilanciato con SMOTE')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.scatter(X_train_dataset.iloc[:, 0], X_train_dataset.iloc[:, 1], marker='o', c=Y_train_dataset,\n",
    "           s=25, edgecolor='k', cmap=plt.cm.coolwarm)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dopo aver ottimizzato i dati, si passa all'estrazione di un modello. Per tale problema si selezionera il modello con prestazioni migliori tra SVM, RandomForest e KNN. \n",
    "Inolte per tali modelli verra selezionata la configurazione migliore tra i seguenti iperparametri:\n",
    "1. ##### SVM Iperparametri\n",
    "    1. C: [0.001,0.01,0.1,1.0,10.0,100.0]\n",
    "    2. Kernel: [linear,rbf]\n",
    "    \n",
    "2. ##### RandomForest Iperparametri:\n",
    "    1. N_Estimator: [20, 50, 100, 150]\n",
    "    2. Criterion: [gini, entropy]\n",
    "    \n",
    "3. ##### KNN Iperparametri:\n",
    "    1. N_neighbors: [1, 10, 20, 30]\n",
    "    2. Weights: [uniform, distance]\n",
    "    3. Algorithm: [auto, ball_tree, kd_tree, brute]\n",
    "    4. Metric: [euclidean, manhattan, minkowski]\n",
    " \n",
    " Per la selezione del miglior modello con la miglior configurazione verra eseguita la cross validation nidificata (10x3) sul dataset di test e verranno mostrate le performance dell'algoritmo\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "def print_scores_cv(scores,label_classifier):\n",
    "    plt.plot(range(10), scores['accuracy'], marker='o', label=\"accuracy\")\n",
    "    plt.plot(range(10), scores['precision'], marker='o', label=\"precision\")\n",
    "    plt.plot(range(10), scores['recall'], marker='o', label=\"recall\")\n",
    "    plt.plot(range(10), scores['f1'], marker='o', label=\"f1\")\n",
    "    plt.axhline(y=np.mean(scores['accuracy']), color='r', linestyle='--', label=\"accuracy mean\")\n",
    "    plt.axhline(y=np.mean(scores['precision']), color='orange', linestyle='--', label=\"precision mean\")\n",
    "    plt.axhline(y=np.mean(scores['recall']), color='g', linestyle='--', label=\"recall mean\")\n",
    "    plt.axhline(y=np.mean(scores['f1']), color='b', linestyle='--', label=\"f1 mean\")\n",
    "\n",
    "    plt.axis([0, 9, 0.7, 1])\n",
    "    plt.title(str('Cross Validation Nidificata: ' + label_classifier))\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.xlabel('10-Fold Iteration')\n",
    "    plt.ylabel('Performance')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "X_train_dst=X_train_dataset.to_numpy()\n",
    "Y_train_dst=Y_train_dataset.to_numpy()\n",
    "\n",
    "\n",
    "clf1=SVC()\n",
    "clf2=RandomForestClassifier()\n",
    "clf3=KNeighborsClassifier()\n",
    "clf_labels=['RandomForest','SVM','KNN']\n",
    "\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10,shuffle=False)\n",
    "\n",
    "\n",
    "scores={\n",
    "    'accuracy':[],\n",
    "    'precision':[],\n",
    "    'recall':[],\n",
    "   'f1':[]\n",
    "}\n",
    "\n",
    "grideSearch_SVM=[\n",
    "    {\n",
    "\t    'C':[0.001,0.01,0.1,1.0,10.0,100.0],\n",
    "\t    'kernel':['linear','rbf'],\n",
    "    }\n",
    "]\n",
    "\n",
    "grideSearch_RF=[\n",
    "    {\n",
    "        'n_estimators': [20, 50, 100, 150],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "    }\n",
    "]\n",
    "\n",
    "grideSearch_KNN=[\n",
    "    {\n",
    "        'n_neighbors': [1, 10, 20, 30],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    }\n",
    "]\n",
    "best_params=[]\n",
    "for clf,label,grid_param in zip([clf2,clf1,clf3],clf_labels,[grideSearch_RF,grideSearch_SVM,grideSearch_KNN]):\n",
    "    best_params.clear()\n",
    "    scores['accuracy'].clear()\n",
    "    scores['precision'].clear()\n",
    "    scores['recall'].clear()\n",
    "    scores['f1'].clear()\n",
    "    for train_index,test_index in skf.split(X=X_train_dst,y=Y_train_dst):\n",
    "        #Creo il set di train e test\n",
    "        X_train, X_test = X_train_dst[train_index], X_train_dst[test_index]\n",
    "        y_train, y_test = Y_train_dst[train_index], Y_train_dst[test_index]\n",
    "        \n",
    "        grid_clf=GridSearchCV(clf,param_grid=grid_param,cv=3,verbose=True,n_jobs=-1)\n",
    "        grid_clf.fit(X=X_train,y=y_train) #Eseguo la grid search sulla parte di training\n",
    "        best_params.append(grid_clf.best_params_)\n",
    "        clf.set_params(**grid_clf.best_params_)\n",
    "        \n",
    "        #Addestro il classificatore sul set di train\n",
    "        clf.fit(X=X_train,y=y_train)\n",
    "        #Testo il classificatore\n",
    "        y_predict=clf.predict(X_test)\n",
    "\n",
    "        #Salvo Accuracy,Precision,Recall,F1\n",
    "        scores['accuracy'].append(accuracy_score(y_true=y_test, y_pred=y_predict))\n",
    "        scores['precision'].append(precision_score(y_true=y_test, y_pred=y_predict))\n",
    "        scores['recall'].append(recall_score(y_true=y_test, y_pred=y_predict))\n",
    "        scores['f1'].append(f1_score(y_true=y_test, y_pred=y_predict))\n",
    "\n",
    "    i=0\n",
    "    for best_param in best_params:\n",
    "        print(\"Parametri \",label,\" iterazione \",i,\" :\",best_param)\n",
    "        i=i+1\n",
    "    print_scores_cv(scores,label)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Il modello che restituisce le performance migliori risulta essere il Random Forest con prestazioni in Accuraci, Precision, Recall e F1 al disopra del 95%. Inoltre la miglior configurazione per il nostro ambiente risulta essere la seguente: \n",
    "{'criterion': 'gini', 'n_estimators': 150}\n",
    "Pertanto il passo successivo e la valutazione del modello sulla partizione di test."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evalutation\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "test_set_copy=test_set.copy()\n",
    "test_dataset=test_set.drop(['id','nameProject','testCase','isFlaky'],axis=1)\n",
    "test_dataset_lable=test_set['isFlaky']\n",
    "test_dataset_lable=test_dataset_lable.astype(int)\n",
    "X_test_dataset=sc.transform(test_dataset)\n",
    "principalCompontent=pca.transform(X_test_dataset)\n",
    "pca_test_dataset=pandas.DataFrame(principalCompontent,columns=['Principal Component 1','Principal Component 2','Principal Component  3',\n",
    "                                                                'Principal Component 4','Principal Component 5','Principal Component 6',\n",
    "                                                                'Principal Component 7','Principal Component 8','Principal Component 9',\n",
    "\n",
    "                                                                'Principal Component 10'])\n",
    "clf=RandomForestClassifier(criterion='gini', n_estimators=150)\n",
    "clf.fit(X_train_dataset,Y_train_dataset)\n",
    "test_predict=clf.predict(pca_test_dataset)\n",
    "confmat=confusion_matrix(y_true=test_dataset_lable,y_pred=test_predict)\n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j,y=i, s=confmat[i,j], va='center', ha='center')\n",
    "plt.xlabel('predict label')\n",
    "plt.ylabel('true label')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
    "plt.bar(['Accuracy','Precision','Recall','F1'],[accuracy_score(y_true=test_dataset_lable,y_pred=test_predict),precision_score(y_true=test_dataset_lable,y_pred=test_predict),recall_score(y_true=test_dataset_lable,y_pred=test_predict),f1_score(y_true=test_dataset_lable,y_pred=test_predict)])\n",
    "plt.show() "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Da come si nota dalla matrice di confusione, il Random Forest riesce a classificare corretta i test non flaky a differenza di quelli flaky. Tuttavia l'obiettivo di business è classificare correttamente i flaky test e tale obiettino si può ritenere non raggiunto.\n",
    "Essendo che sia SVM e KNN hanno delle ottime prestazioni, sebbene inferiori al Random Forest, per cercare di aumentare precision e recall proveremo a creare un meta classificatore combinando questi 3 modelli.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from MajorityVoteClassifier import MajorityVoteClassifier\n",
    "\n",
    "clf1=SVC(C=100.0, kernel='rbf')\n",
    "clf2=RandomForestClassifier(criterion='gini', n_estimators=150)\n",
    "clf3=KNeighborsClassifier(algorithm='brute', metric='euclidean', n_neighbors=1, weights='uniform')\n",
    "ensemble=MajorityVoteClassifier(classifiers=[clf1,clf2,clf3])\n",
    "ensemble.fit(X_train_dataset,Y_train_dataset)\n",
    "test_predict=ensemble.predict(pca_test_dataset)\n",
    "confmat=confusion_matrix(y_true=test_dataset_lable,y_pred=test_predict)\n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j,y=i, s=confmat[i,j], va='center', ha='center')\n",
    "plt.xlabel('predict label')\n",
    "plt.ylabel('true label')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
    "plt.bar(['Accuracy','Precision','Recall','F1'],[accuracy_score(y_true=test_dataset_lable,y_pred=test_predict),precision_score(y_true=test_dataset_lable,y_pred=test_predict),recall_score(y_true=test_dataset_lable,y_pred=test_predict),f1_score(y_true=test_dataset_lable,y_pred=test_predict)])\n",
    "plt.show() "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utilizzando l ensamble dei 3 modelli, abbiamo ottenuto dei leggeri miglioramenti in termini di Recall e F1, tuttavia le prestazioni non sono ancora ottime.\n",
    "Un tale risultato era abbastanza scontato in quando nel dataset abbio pochissimi campioni di test flaky. Tale aspetto e sicuramente uno svantaggio per il machine learning poiche un modello riescere ad apprendere bene lavorando so molti dati. Tuttavia risulta vero che tale problematica viene risolta attraverso SMOTE creando dei test flaky in modo da bilanciare il problema, pero la problematica si risolve solamente in parete. \n",
    "Ovvero nel momento che eseguiamo lo split del dominio(dataset) in train e test, esso non è ancora bilanciato pertanto possono rientrare dei cosiddetti casi limiti nella test set. Utilizzando SMOTE per bilanciare il train set andremo ad aggiungere dei test flaky per poter riconoscere meglio quelli presenti in tale set, tuttavia  per tutti i casi limiti rientranti nel train set non si ha nessuna certezza nel avere predizioni corrette.\n",
    "Una soluzione a tale problema e eliminare il partizionamento dei dati e eseguire il train del modello sull'intero dataset.\n",
    "\n",
    "Diciamo che lo scopo di avere un test-set è solamente quello di conoscere le effettive prestazioni del modello su dati che non conosce, ma come ribadito precedente mente eseguire uno split in piccoli dataset non bilanciati può risultare un grosso problema. Con l'utilizzo della cross validation possiamo conoscere le reali prestazioni dell modello nell ambiente anche senza utilizzare il test-set. Inoltre con la cross validation nidificate non solo possiamo conoscere le reali prestazioni del modello nell'ambiente ma anche la sua migliore configurazione.\n",
    "\n",
    "Pertanto si è deciso di utilizzare tale approccio per il nostro problema. Verra saltata la fase di partizionamento dei dati, la fase di feature engineering sara identica alla precedente mentre per la fase di selezione del modello verrra utilizzata una cross validation nidificata 10x3 sull'intero dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross-Validation Nidificata"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset=loadingDataSet(DATASET_NAME)\n",
    "dataset_copy=dataset.copy()\n",
    "dataset_noLable=dataset.drop(['id','nameProject','testCase','isFlaky'],axis=1)\n",
    "dataset_lable=dataset['isFlaky']\n",
    "dataset_lable=dataset_lable.astype(int)\n",
    "sc=StandardScaler()\n",
    "X_dataset=sc.fit_transform(dataset_noLable)\n",
    "pca=PCA(n_components=10)\n",
    "principalCompontent=pca.fit_transform(X_dataset)\n",
    "pca_dataset=pandas.DataFrame(principalCompontent,columns=['Principal Component 1','Principal Component 2','Principal Component  3',\n",
    "                                                                'Principal Component 4','Principal Component 5','Principal Component 6',\n",
    "                                                                'Principal Component 7','Principal Component 8','Principal Component 9',\n",
    "                                                                'Principal Component 10'])\n",
    "sm=SMOTE(sampling_strategy='auto', k_neighbors=1,random_state=42)\n",
    "X_dataset,Y_dataset=sm.fit_resample(pca_dataset,dataset_lable)\n",
    "X_dataset=X_dataset.to_numpy()\n",
    "Y_dataset=Y_dataset.to_numpy()\n",
    "\n",
    "\n",
    "clf1=SVC()\n",
    "clf2=RandomForestClassifier()\n",
    "clf3=KNeighborsClassifier()\n",
    "clf_labels=['RandomForest','SVM','KNN']\n",
    "\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10,shuffle=False)\n",
    "\n",
    "\n",
    "scores={\n",
    "    'accuracy':[],\n",
    "    'precision':[],\n",
    "    'recall':[],\n",
    "   'f1':[]\n",
    "}\n",
    "\n",
    "grideSearch_SVM=[\n",
    "    {\n",
    "\t    'C':[0.001,0.01,0.1,1.0,10.0,100.0],\n",
    "\t    'kernel':['linear','rbf'],\n",
    "    }\n",
    "]\n",
    "\n",
    "grideSearch_RF=[\n",
    "    {\n",
    "        'n_estimators': [20, 50, 100, 150],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "    }\n",
    "]\n",
    "\n",
    "grideSearch_KNN=[\n",
    "    {\n",
    "        'n_neighbors': [1, 10, 20, 30],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    }\n",
    "]\n",
    "best_params=[]\n",
    "for clf,label,grid_param in zip([clf2,clf1,clf3],clf_labels,[grideSearch_RF,grideSearch_SVM,grideSearch_KNN]):\n",
    "    best_params.clear()\n",
    "    scores['accuracy'].clear()\n",
    "    scores['precision'].clear()\n",
    "    scores['recall'].clear()\n",
    "    scores['f1'].clear()\n",
    "    for train_index,test_index in skf.split(X=X_dataset,y=Y_dataset):\n",
    "        #Creo il set di train e test\n",
    "        X_train, X_test = X_dataset[train_index], X_dataset[test_index]\n",
    "        y_train, y_test = Y_dataset[train_index], Y_dataset[test_index]\n",
    "        \n",
    "        grid_clf=GridSearchCV(clf,param_grid=grid_param,cv=3,scoring='average_precision',verbose=True,n_jobs=-1)\n",
    "        grid_clf.fit(X=X_train,y=y_train) #Eseguo la grid search sulla parte di training\n",
    "        best_params.append(grid_clf.best_params_)\n",
    "        clf.set_params(**grid_clf.best_params_)\n",
    "        \n",
    "        #Addestro il classificatore sul set di train\n",
    "        clf.fit(X=X_train,y=y_train)\n",
    "        #Testo il classificatore\n",
    "        y_predict=clf.predict(X_test)\n",
    "\n",
    "        #Salvo Accuracy,Precision,Recall,F1\n",
    "        scores['accuracy'].append(accuracy_score(y_true=y_test, y_pred=y_predict))\n",
    "        scores['precision'].append(precision_score(y_true=y_test, y_pred=y_predict))\n",
    "        scores['recall'].append(recall_score(y_true=y_test, y_pred=y_predict))\n",
    "        scores['f1'].append(f1_score(y_true=y_test, y_pred=y_predict))\n",
    "\n",
    "    i=0\n",
    "    for best_param in best_params:\n",
    "        print(\"Parametri \",label,\" iterazione \",i,\" :\",best_param)\n",
    "        i=i+1\n",
    "    print_scores_cv(scores,label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "L'algoritmo con le prestazioni migliori risulta essere ancora il Random Forest, dove nella media si avranno prestazioni al disopra del 90% e nel caso peggiore si abbasseranno all 80%.\n",
    "La sua migliore configurazione per il nostro ambiente risulta essere {'criterion': 'gini', 'n_estimators': 100} oppure {'criterion': 'entropy', 'n_estimators': 100}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utilizzando l ensamble dei 3 modelli, abbiamo ottenuto dei leggeri miglioramenti in termini di Recall e F1, tuttavia le prestazioni non sono ancora ottime.\n",
    "Un tale risultato era abbastanza scontato in quando nel dataset abbio pochissimi campioni di test flaky. Tale aspetto e sicuramente uno svantaggio per il machine learning poiche un modello riescere ad apprendere bene lavorando so molti dati. Tuttavia risulta vero che tale problematica viene risolta attraverso SMOTE creando dei test flaky in modo da bilanciare il problema, pero la problematica si risolve solamente in parete. \n",
    "Ovvero nel momento che eseguiamo lo split del dominio(dataset) in train e test, esso non è ancora bilanciato pertanto possono rientrare dei cosiddetti casi limiti nella test set. Utilizzando SMOTE per bilanciare il train set andremo ad aggiungere dei test flaky per poter riconoscere meglio quelli presenti in tale set, tuttavia  per tutti i casi limiti rientranti nel train set non si ha nessuna certezza nel avere predizioni corrette.\n",
    "Una soluzione a tale problema e eliminare il partizionamento dei dati e eseguire il train del modello sull'intero dataset.\n",
    "\n",
    "Diciamo che lo scopo di avere un test-set è solamente quello di conoscere le effettive prestazioni del modello su dati che non conosce, ma come ribadito precedente mente eseguire uno split in piccoli dataset non bilanciati può risultare un grosso problema. Con l'utilizzo della cross validation possiamo conoscere le reali prestazioni dell modello nell ambiente anche senza utilizzare il test-set. Inoltre con la cross validation nidificate non solo possiamo conoscere le reali prestazioni del modello nell'ambiente ma anche la sua migliore configurazione.\n",
    "\n",
    "Pertanto si è deciso di utilizzare tale approccio per il nostro problema. Verra saltata la fase di partizionamento dei dati, la fase di feature engineering sara identica alla precedente mentre per la fase di selezione del modello verrra utilizzata una cross validation nidificata 10x3 sull'intero dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross-Validation Nidificata"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset=loadingDataSet(DATASET_NAME)\n",
    "dataset_copy=dataset.copy()\n",
    "dataset_noLable=dataset.drop(['id','nameProject','testCase','isFlaky'],axis=1)\n",
    "dataset_lable=dataset['isFlaky']\n",
    "dataset_lable=dataset_lable.astype(int)\n",
    "sc=StandardScaler()\n",
    "X_dataset=sc.fit_transform(dataset_noLable)\n",
    "pca=PCA(n_components=10)\n",
    "principalCompontent=pca.fit_transform(X_dataset)\n",
    "pca_dataset=pandas.DataFrame(principalCompontent,columns=['Principal Component 1','Principal Component 2','Principal Component  3',\n",
    "                                                                'Principal Component 4','Principal Component 5','Principal Component 6',\n",
    "                                                                'Principal Component 7','Principal Component 8','Principal Component 9',\n",
    "                                                                'Principal Component 10'])\n",
    "sm=SMOTE(sampling_strategy='auto', k_neighbors=1,random_state=42)\n",
    "X_dataset,Y_dataset=sm.fit_resample(pca_dataset,dataset_lable)\n",
    "X_dataset=X_dataset.to_numpy()\n",
    "Y_dataset=Y_dataset.to_numpy()\n",
    "\n",
    "\n",
    "clf1=SVC()\n",
    "clf2=RandomForestClassifier()\n",
    "clf3=KNeighborsClassifier()\n",
    "clf_labels=['RandomForest','SVM','KNN']\n",
    "\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10,shuffle=False)\n",
    "\n",
    "\n",
    "scores={\n",
    "    'accuracy':[],\n",
    "    'precision':[],\n",
    "    'recall':[],\n",
    "   'f1':[]\n",
    "}\n",
    "\n",
    "grideSearch_SVM=[\n",
    "    {\n",
    "\t    'C':[0.001,0.01,0.1,1.0,10.0,100.0],\n",
    "\t    'kernel':['linear','rbf'],\n",
    "    }\n",
    "]\n",
    "\n",
    "grideSearch_RF=[\n",
    "    {\n",
    "        'n_estimators': [20, 50, 100, 150],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "    }\n",
    "]\n",
    "\n",
    "grideSearch_KNN=[\n",
    "    {\n",
    "        'n_neighbors': [1, 10, 20, 30],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    }\n",
    "]\n",
    "best_params=[]\n",
    "for clf,label,grid_param in zip([clf2,clf1,clf3],clf_labels,[grideSearch_RF,grideSearch_SVM,grideSearch_KNN]):\n",
    "    best_params.clear()\n",
    "    scores['accuracy'].clear()\n",
    "    scores['precision'].clear()\n",
    "    scores['recall'].clear()\n",
    "    scores['f1'].clear()\n",
    "    for train_index,test_index in skf.split(X=X_dataset,y=Y_dataset):\n",
    "        #Creo il set di train e test\n",
    "        X_train, X_test = X_dataset[train_index], X_dataset[test_index]\n",
    "        y_train, y_test = Y_dataset[train_index], Y_dataset[test_index]\n",
    "        \n",
    "        grid_clf=GridSearchCV(clf,param_grid=grid_param,cv=3,scoring='average_precision',verbose=True,n_jobs=-1)\n",
    "        grid_clf.fit(X=X_train,y=y_train) #Eseguo la grid search sulla parte di training\n",
    "        best_params.append(grid_clf.best_params_)\n",
    "        clf.set_params(**grid_clf.best_params_)\n",
    "        \n",
    "        #Addestro il classificatore sul set di train\n",
    "        clf.fit(X=X_train,y=y_train)\n",
    "        #Testo il classificatore\n",
    "        y_predict=clf.predict(X_test)\n",
    "\n",
    "        #Salvo Accuracy,Precision,Recall,F1\n",
    "        scores['accuracy'].append(accuracy_score(y_true=y_test, y_pred=y_predict))\n",
    "        scores['precision'].append(precision_score(y_true=y_test, y_pred=y_predict))\n",
    "        scores['recall'].append(recall_score(y_true=y_test, y_pred=y_predict))\n",
    "        scores['f1'].append(f1_score(y_true=y_test, y_pred=y_predict))\n",
    "\n",
    "    i=0\n",
    "    for best_param in best_params:\n",
    "        print(\"Parametri \",label,\" iterazione \",i,\" :\",best_param)\n",
    "        i=i+1\n",
    "    print_scores_cv(scores,label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "L'algoritmo con le prestazioni migliori risulta essere ancora il Random Forest, dove nella media si avranno prestazioni al disopra del 90% e nel caso peggiore si abbasseranno all 80%.\n",
    "La sua migliore configurazione per il nostro ambiente risulta essere {'criterion': 'gini', 'n_estimators': 100} oppure {'criterion': 'entropy', 'n_estimators': 100}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utilizzando l ensamble dei 3 modelli, abbiamo ottenuto dei leggeri miglioramenti in termini di Recall e F1, tuttavia le prestazioni non sono ancora ottime.\n",
    "Un tale risultato era abbastanza scontato in quando nel dataset abbio pochissimi campioni di test flaky. Tale aspetto e sicuramente uno svantaggio per il machine learning poiche un modello riescere ad apprendere bene lavorando so molti dati. Tuttavia risulta vero che tale problematica viene risolta attraverso SMOTE creando dei test flaky in modo da bilanciare il problema, pero la problematica si risolve solamente in parete. \n",
    "Ovvero nel momento che eseguiamo lo split del dominio(dataset) in train e test, esso non è ancora bilanciato pertanto possono rientrare dei cosiddetti casi limiti nella test set. Utilizzando SMOTE per bilanciare il train set andremo ad aggiungere dei test flaky per poter riconoscere meglio quelli presenti in tale set, tuttavia  per tutti i casi limiti rientranti nel train set non si ha nessuna certezza nel avere predizioni corrette.\n",
    "Una soluzione a tale problema e eliminare il partizionamento dei dati e eseguire il train del modello sull'intero dataset.\n",
    "\n",
    "Diciamo che lo scopo di avere un test-set è solamente quello di conoscere le effettive prestazioni del modello su dati che non conosce, ma come ribadito precedente mente eseguire uno split in piccoli dataset non bilanciati può risultare un grosso problema. Con l'utilizzo della cross validation possiamo conoscere le reali prestazioni dell modello nell ambiente anche senza utilizzare il test-set. Inoltre con la cross validation nidificate non solo possiamo conoscere le reali prestazioni del modello nell'ambiente ma anche la sua migliore configurazione.\n",
    "\n",
    "Pertanto si è deciso di utilizzare tale approccio per il nostro problema. Verra saltata la fase di partizionamento dei dati, la fase di feature engineering sara identica alla precedente mentre per la fase di selezione del modello verrra utilizzata una cross validation nidificata 10x3 sull'intero dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross-Validation Nidificata"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset=loadingDataSet(DATASET_NAME)\n",
    "dataset_copy=dataset.copy()\n",
    "dataset_noLable=dataset.drop(['id','nameProject','testCase','isFlaky'],axis=1)\n",
    "dataset_lable=dataset['isFlaky']\n",
    "dataset_lable=dataset_lable.astype(int)\n",
    "sc=StandardScaler()\n",
    "X_dataset=sc.fit_transform(dataset_noLable)\n",
    "pca=PCA(n_components=10)\n",
    "principalCompontent=pca.fit_transform(X_dataset)\n",
    "pca_dataset=pandas.DataFrame(principalCompontent,columns=['Principal Component 1','Principal Component 2','Principal Component  3',\n",
    "                                                                'Principal Component 4','Principal Component 5','Principal Component 6',\n",
    "                                                                'Principal Component 7','Principal Component 8','Principal Component 9',\n",
    "                                                                'Principal Component 10'])\n",
    "sm=SMOTE(sampling_strategy='auto', k_neighbors=1,random_state=42)\n",
    "X_dataset,Y_dataset=sm.fit_resample(pca_dataset,dataset_lable)\n",
    "X_dataset=X_dataset.to_numpy()\n",
    "Y_dataset=Y_dataset.to_numpy()\n",
    "\n",
    "\n",
    "clf1=SVC()\n",
    "clf2=RandomForestClassifier()\n",
    "clf3=KNeighborsClassifier()\n",
    "clf_labels=['RandomForest','SVM','KNN']\n",
    "\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10,shuffle=False)\n",
    "\n",
    "\n",
    "scores={\n",
    "    'accuracy':[],\n",
    "    'precision':[],\n",
    "    'recall':[],\n",
    "   'f1':[]\n",
    "}\n",
    "\n",
    "grideSearch_SVM=[\n",
    "    {\n",
    "\t    'C':[0.001,0.01,0.1,1.0,10.0,100.0],\n",
    "\t    'kernel':['linear','rbf'],\n",
    "    }\n",
    "]\n",
    "\n",
    "grideSearch_RF=[\n",
    "    {\n",
    "        'n_estimators': [20, 50, 100, 150],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "    }\n",
    "]\n",
    "\n",
    "grideSearch_KNN=[\n",
    "    {\n",
    "        'n_neighbors': [1, 10, 20, 30],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    }\n",
    "]\n",
    "best_params=[]\n",
    "for clf,label,grid_param in zip([clf2,clf1,clf3],clf_labels,[grideSearch_RF,grideSearch_SVM,grideSearch_KNN]):\n",
    "    best_params.clear()\n",
    "    scores['accuracy'].clear()\n",
    "    scores['precision'].clear()\n",
    "    scores['recall'].clear()\n",
    "    scores['f1'].clear()\n",
    "    for train_index,test_index in skf.split(X=X_dataset,y=Y_dataset):\n",
    "        #Creo il set di train e test\n",
    "        X_train, X_test = X_dataset[train_index], X_dataset[test_index]\n",
    "        y_train, y_test = Y_dataset[train_index], Y_dataset[test_index]\n",
    "        \n",
    "        grid_clf=GridSearchCV(clf,param_grid=grid_param,cv=3,scoring='average_precision',verbose=True,n_jobs=-1)\n",
    "        grid_clf.fit(X=X_train,y=y_train) #Eseguo la grid search sulla parte di training\n",
    "        best_params.append(grid_clf.best_params_)\n",
    "        clf.set_params(**grid_clf.best_params_)\n",
    "        \n",
    "        #Addestro il classificatore sul set di train\n",
    "        clf.fit(X=X_train,y=y_train)\n",
    "        #Testo il classificatore\n",
    "        y_predict=clf.predict(X_test)\n",
    "\n",
    "        #Salvo Accuracy,Precision,Recall,F1\n",
    "        scores['accuracy'].append(accuracy_score(y_true=y_test, y_pred=y_predict))\n",
    "        scores['precision'].append(precision_score(y_true=y_test, y_pred=y_predict))\n",
    "        scores['recall'].append(recall_score(y_true=y_test, y_pred=y_predict))\n",
    "        scores['f1'].append(f1_score(y_true=y_test, y_pred=y_predict))\n",
    "\n",
    "    i=0\n",
    "    for best_param in best_params:\n",
    "        print(\"Parametri \",label,\" iterazione \",i,\" :\",best_param)\n",
    "        i=i+1\n",
    "    print_scores_cv(scores,label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "L'algoritmo con le prestazioni migliori risulta essere ancora il Random Forest, dove nella media si avranno prestazioni al disopra del 90% e nel caso peggiore si abbasseranno all 80%.\n",
    "La sua migliore configurazione per il nostro ambiente risulta essere {'criterion': 'gini', 'n_estimators': 100} oppure {'criterion': 'entropy', 'n_estimators': 100}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "e2024fd7-52ba-4ec8-9a69-f20e7e7984e1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Utilizzando l ensamble dei 3 modelli, abbiamo ottenuto dei leggeri miglioramenti in termini di Recall e F1, tuttavia le prestazioni non sono ancora ottime.\n",
    "Un tale risultato era abbastanza scontato in quando nel dataset abbio pochissimi campioni di test flaky. Tale aspetto e sicuramente uno svantaggio per il machine learning poiche un modello riescere ad apprendere bene lavorando so molti dati. Tuttavia risulta vero che tale problematica viene risolta attraverso SMOTE creando dei test flaky in modo da bilanciare il problema, pero la problematica si risolve solamente in parete. \n",
    "Ovvero nel momento che eseguiamo lo split del dominio(dataset) in train e test, esso non è ancora bilanciato pertanto possono rientrare dei cosiddetti casi limiti nella test set. Utilizzando SMOTE per bilanciare il train set andremo ad aggiungere dei test flaky per poter riconoscere meglio quelli presenti in tale set, tuttavia  per tutti i casi limiti rientranti nel train set non si ha nessuna certezza nel avere predizioni corrette.\n",
    "Una soluzione a tale problema e eliminare il partizionamento dei dati e eseguire il train del modello sull'intero dataset.\n",
    "\n",
    "Diciamo che lo scopo di avere un test-set è solamente quello di conoscere le effettive prestazioni del modello su dati che non conosce, ma come ribadito precedente mente eseguire uno split in piccoli dataset non bilanciati può risultare un grosso problema. Con l'utilizzo della cross validation possiamo conoscere le reali prestazioni dell modello nell ambiente anche senza utilizzare il test-set. Inoltre con la cross validation nidificate non solo possiamo conoscere le reali prestazioni del modello nell'ambiente ma anche la sua migliore configurazione.\n",
    "\n",
    "Pertanto si è deciso di utilizzare tale approccio per il nostro problema. Verra saltata la fase di partizionamento dei dati, la fase di feature engineering sara identica alla precedente mentre per la fase di selezione del modello verrra utilizzata una cross validation nidificata 10x3 sull'intero dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc44831-b898-466c-8df3-de77d9ac591a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cross-Validation Nidificata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1bff3-1e9f-426d-a13f-3618b566c60b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset=loadingDataSet(DATASET_NAME)\n",
    "dataset_copy=dataset.copy()\n",
    "dataset_noLable=dataset.drop(['id','nameProject','testCase','isFlaky'],axis=1)\n",
    "dataset_lable=dataset['isFlaky']\n",
    "dataset_lable=dataset_lable.astype(int)\n",
    "sc=StandardScaler()\n",
    "X_dataset=sc.fit_transform(dataset_noLable)\n",
    "pca=PCA(n_components=10)\n",
    "principalCompontent=pca.fit_transform(X_dataset)\n",
    "pca_dataset=pandas.DataFrame(principalCompontent,columns=['Principal Component 1','Principal Component 2','Principal Component  3',\n",
    "                                                                'Principal Component 4','Principal Component 5','Principal Component 6',\n",
    "                                                                'Principal Component 7','Principal Component 8','Principal Component 9',\n",
    "                                                                'Principal Component 10'])\n",
    "sm=SMOTE(sampling_strategy='auto', k_neighbors=1,random_state=42)\n",
    "X_dataset,Y_dataset=sm.fit_resample(pca_dataset,dataset_lable)\n",
    "X_dataset=X_dataset.to_numpy()\n",
    "Y_dataset=Y_dataset.to_numpy()\n",
    "\n",
    "\n",
    "clf1=SVC()\n",
    "clf2=RandomForestClassifier()\n",
    "clf3=KNeighborsClassifier()\n",
    "clf_labels=['RandomForest','SVM','KNN']\n",
    "\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10,shuffle=False)\n",
    "\n",
    "\n",
    "scores={\n",
    "    'accuracy':[],\n",
    "    'precision':[],\n",
    "    'recall':[],\n",
    "   'f1':[]\n",
    "}\n",
    "\n",
    "grideSearch_SVM=[\n",
    "    {\n",
    "\t    'C':[0.001,0.01,0.1,1.0,10.0,100.0],\n",
    "\t    'kernel':['linear','rbf'],\n",
    "    }\n",
    "]\n",
    "\n",
    "grideSearch_RF=[\n",
    "    {\n",
    "        'n_estimators': [20, 50, 100, 150],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "    }\n",
    "]\n",
    "\n",
    "grideSearch_KNN=[\n",
    "    {\n",
    "        'n_neighbors': [1, 10, 20, 30],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    }\n",
    "]\n",
    "best_params=[]\n",
    "for clf,label,grid_param in zip([clf2,clf1,clf3],clf_labels,[grideSearch_RF,grideSearch_SVM,grideSearch_KNN]):\n",
    "    best_params.clear()\n",
    "    scores['accuracy'].clear()\n",
    "    scores['precision'].clear()\n",
    "    scores['recall'].clear()\n",
    "    scores['f1'].clear()\n",
    "    for train_index,test_index in skf.split(X=X_dataset,y=Y_dataset):\n",
    "        #Creo il set di train e test\n",
    "        X_train, X_test = X_dataset[train_index], X_dataset[test_index]\n",
    "        y_train, y_test = Y_dataset[train_index], Y_dataset[test_index]\n",
    "        \n",
    "        grid_clf=GridSearchCV(clf,param_grid=grid_param,cv=3,scoring='average_precision',verbose=True,n_jobs=-1)\n",
    "        grid_clf.fit(X=X_train,y=y_train) #Eseguo la grid search sulla parte di training\n",
    "        best_params.append(grid_clf.best_params_)\n",
    "        clf.set_params(**grid_clf.best_params_)\n",
    "        \n",
    "        #Addestro il classificatore sul set di train\n",
    "        clf.fit(X=X_train,y=y_train)\n",
    "        #Testo il classificatore\n",
    "        y_predict=clf.predict(X_test)\n",
    "\n",
    "        #Salvo Accuracy,Precision,Recall,F1\n",
    "        scores['accuracy'].append(accuracy_score(y_true=y_test, y_pred=y_predict))\n",
    "        scores['precision'].append(precision_score(y_true=y_test, y_pred=y_predict))\n",
    "        scores['recall'].append(recall_score(y_true=y_test, y_pred=y_predict))\n",
    "        scores['f1'].append(f1_score(y_true=y_test, y_pred=y_predict))\n",
    "\n",
    "    i=0\n",
    "    for best_param in best_params:\n",
    "        print(\"Parametri \",label,\" iterazione \",i,\" :\",best_param)\n",
    "        i=i+1\n",
    "    print_scores_cv(scores,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af51de95-2600-45bb-acb7-1cfedf34ca95",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "L'algoritmo con le prestazioni migliori risulta essere ancora il Random Forest, dove nella media si avranno prestazioni al disopra del 90% e nel caso peggiore si abbasseranno all 80%.\n",
    "La sua migliore configurazione per il nostro ambiente risulta essere {'criterion': 'gini', 'n_estimators': 100} oppure {'criterion': 'entropy', 'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6579598-8f33-4fca-91ea-1dcac819e536",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}